{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'x_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mvap\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransformer\u001b[39;00m \u001b[39mimport\u001b[39;00m VapStereoTower\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mvap\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mencoder\u001b[39;00m \u001b[39mimport\u001b[39;00m EncoderCPC\n",
      "File \u001b[0;32m~/Desktop/VoiceActivityProjection/vap/modules/transformer.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctools\u001b[39;00m \u001b[39mimport\u001b[39;00m partial\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mx_transformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mx_transformers\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_device\u001b[39m(layer: nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m     10\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(layer\u001b[39m.\u001b[39mparameters())\u001b[39m.\u001b[39mdevice\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'x_transformers'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from vap.modules.transformer import VapStereoTower\n",
    "from vap.modules.encoder import EncoderCPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCPC(nn.Module):\n",
    "    def __init__(self, output_dims=256):\n",
    "        super().__init__()\n",
    "        self.output_dims = output_dims\n",
    "        self.conv = nn.Conv1d(1, output_dims, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assuming x shape is (batch, channels, samples)\n",
    "        return self.conv(x)\n",
    "\n",
    "class VapStereoTower(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=dim, nhead=num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x1, x2, return_attn=False):\n",
    "        # Dummy implementation for attention output\n",
    "        attn_weights = []\n",
    "        for layer in self.layers:\n",
    "            x1 = layer(x1)\n",
    "            attn = torch.randn(x1.size(0), x1.size(1), x1.size(1))  # Simulating attention weights\n",
    "            attn_weights.append(attn)\n",
    "        if return_attn:\n",
    "            return x1, attn_weights\n",
    "        return x1\n",
    "\n",
    "class VAP(nn.Module):\n",
    "    def __init__(self, encoder, transformer):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2 = self.encoder(x[:, 0:1]), self.encoder(x[:, 1:2])  # Encode each channel\n",
    "        _, attn_weights = self.transformer(x1, x2, return_attn=True)\n",
    "        return attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model components\n",
    "encoder = EncoderCPC(output_dims=256)\n",
    "transformer = VapStereoTower(dim=256, num_heads=8, num_layers=1)\n",
    "\n",
    "# Instantiate the VAP model\n",
    "model = VAP(encoder, transformer)\n",
    "\n",
    "# Generate dummy stereo audio signal\n",
    "dummy_audio = torch.rand(1, 2, 16000)  # Batch size, channels, samples\n",
    "\n",
    "# Forward pass to get attention weights\n",
    "attention_weights = model(dummy_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming attention_weights is a list of tensors, where each tensor is [batch_size, num_queries, num_keys]\n",
    "# Visualize the attention weights for the first head of the first layer\n",
    "sns.heatmap(attention_weights[0][0].detach().numpy(), cmap='viridis', annot=True)\n",
    "plt.title('Attention Map')\n",
    "plt.xlabel('Keys')\n",
    "plt.ylabel('Queries')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
